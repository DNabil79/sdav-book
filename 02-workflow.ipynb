{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 ANALYTICAL WORKFLOWS\n",
    "\n",
    "## An Analytical Workflow\n",
    "\n",
    "What is the workflow required for performing cyber security data analytics? We can consider a 5-stage pipeline:\n",
    "\n",
    "- **Collect:** What form of data are we planning to collect, and where are we collecting this from? This is a fundamental stage in any analytics task, as it forces us to think what is the problem that we plan to address, or the story that we plan to tell, and how do we gather appropriate data to best inform this. Typically, we require some form of sensor for data collection. This may well be some form of software-based sensor, such as a Python script that logs every new login for a given device, or logs the processes running on a workstation. It may also be a physical sensor, such as an Internet of Things (IoT) device that is capable of gathering some real-world measurement or observation. Humans can also serve as sensors, for example, reporting on some perceived state of the world, such as employee engagement.\n",
    "- **Store:** How do we store this data? Many different data storage methods will exist. Common file data formats include Comma Separated Values (CSV) and Javascript Object Notation (JSON). Databases are widely used in complex systems, and typically either hold structured or unstructured data. Sequel (SQL) is a widely used structured database format, whilst NoSQL unstructured methods such as MongoDB have been adopted more recently.\n",
    "- **Combine / Transform:** Once we have a data store, what are we going to do with it? Typically, we are looking to either combine multiple data sources together to inform a particular observation, or we are looking to transform data in some manner to make it easier to explore and understand. [Aggregation](https://en.wikipedia.org/wiki/Aggregate_function) is widely used for transforming data. To aggregate data simply means to perform some operation such that the data is reduced to some form of summary; a common example would be to take the mean value of the data over a given time period. For example, given raw packet capture data, what is the mean number of packets observed every minute? Calculating this would give us a much better indication of the amount of activity over time compared to studying raw packet capture data. Often, this can be described as [feature extraction](https://en.wikipedia.org/wiki/Feature_extraction) when deriving features that will be used to inform some machine learning model. It may also be described as [data pre-processing](https://hackernoon.com/what-steps-should-one-take-while-doing-data-preprocessing-502c993e1caa), since we are essentially preparing our data so that it is a much appropriate format for informing our investigation. Examples may include rescaling absolute values, such as rescaling RGB pixel values or rescaling RAM usage to be a percentage.\n",
    "- **Analyse:** We now have our raw data, and we have a feature-based representation of the data, and so next is to establish what form of analyse we wish to conduct. We may wish to examine data based on some thresholding techniques – for example, identify all cases where more than X network connections have been established during a 1-hour period. We may also wish to examine data using [signature-based techniques](https://ieeexplore.ieee.org/document/8716381) – for example, identify all software executables where the MD5 hash matches against a known set of examples (often described as a dictionary, and widely used for anti-virus detection). There are other more sophisticated forms of analysis we may also want to consider. We may want to perform [classification](https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623) of our data, where we have a set of possible outcomes or groups, and we want to identify which group each instance of our data belongs to (e.g., classification of malware family samples). Another form of analysis that we may wish to perform is [forecasting](https://machinelearningmastery.com/findings-comparing-classical-and-machine-learning-methods-for-time-series-forecasting/). For example, if we have observed 2 data breaches in the last 12-month period, can we forecast whether we will fall victim again, based on possible mitigations that could be deployed. [Clustering](https://towardsdatascience.com/unsupervised-machine-learning-clustering-analysis-d40f2b34ae7e) is another form that is widely used because it allows us to group similar observations together, to then examine whether they truly are similar. This sounds similar to classification, however in classification we have labelled samples to train on, whereas in clustering we do not have any labels. We will discuss this further in Chapter 4. Finally, there is outlier and anomaly detection – a key part of security analysis. Given some observation, how can I identify when something is different, and more specifically, under what conditions is it anomalous. We will describe this in further detail in due course.\n",
    "- **Visualise:** Following our analysis, we need to plan how to communicate our findings. We may use 2-dimensional charts and plots to do this, such as line plots, bar charts, scatter plots, as well as other forms of visual plots. It’s important to appreciate that numerical values in a table also communicate data – sometimes a table of numerical values will perform more effectively than a visualisation, often if the data is small. As the data becomes larger however, visualisation helps to summarise this and convey details clearer. Depending on the task, we may find that 3-dimensional visualisation is appropriate. This could be a 3-dimensional plot, or it could be a 3-dimensional reconstruction of some scene, depending on the application. Importantly, we need to think about how the end-user will receive information from the chosen representation. Focus-and-context is a common technique to allow users to examine some aspect in detail, whilst also showing how this fits within the broader data. More and more nowadays, interactive techniques in visualisation need to be considered, for how a user will learn and understand the data through interaction, such as parameter adjustment, or zoom and filter of the data.\n",
    "\n",
    "## Data Science Workflows\n",
    "\n",
    "Whilst we have described a possible workflow above, it’s important to recognise that there is no single right way of doing data science. More specifically, if we keep in mind that we are doing analysis for a purpose, to help inform a story, whether that be exploratory or explanatory, we need to establish the question that we are asking, so that we can strive to answer this.\n",
    "\n",
    "![Alt text](./images/image4.png)\n",
    "\n",
    "Here we see another possible workflow, where we decide on a question, and we acquire data to support this question. We clean and transform this data, and we perform our data analysis – much like we have discussed already. However, it is important to recognise that this is not a single one-time process, nor we will necessary ask the right question to being with. In this example, we see three “loops” back to earlier stages of the process – we may need to update our analysis, for example, if we decide that we need to consider more or different features about the data. We may examine our output and present our results, to find that we need more data – for example, if we are examining the presence of an insider threat, do we have sufficient information about the actions they have taken, or do we only have coverage of a subset (e.g., we may have network activity, but we have no record of file access activity). Finally, we may decide that we need to update our question, for example, is our original question not achievable, or do we need to be more specific with how the question is formed?\n",
    "\n",
    "Two further models of analysis are presented here – both showing how analysis is very much an iterative process that will continually evolve. (Further detail in the book by [D. Keim “Mastering the Information Age: Solving Problems with Visual Analytics, 2010](https://www.vismaster.eu/wp-content/uploads/2010/11/VisMaster-book-lowres.pdf)) In the literature, often researchers will describe this as having a “human-in-the-loop” or by adopting a “visual analytics loop”. This is more and more common nowadays as dashboards and interactive analysis techniques have become a normal means of practice.\n",
    "\n",
    "![Alt text](./images/image5.png)\n",
    "![Alt text](./images/image6.png)\n",
    "\n",
    "## Introducing Python Notebooks\n",
    "\n",
    "Notebooks are used for rapid prototyping of ideas, to test a theory or to derive some experiment. In particular, when we talk of notebooks we are describing computational notebooks, where we want to jot down ideas, and test these, whilst being able to jot down code samples and examine our results and findings quickly. Scripting languages such as Python and Javascript make it much easier to rapidly construct a piece of code to process some data, rather than having to compile code to source, like you would do in C or Java. Notebook environments provide a clean approach for integrating code, text, graphics, and other forms of media, using a web browser to interact with this rather than relying on traditional IDEs or text editors. Notebooks stem back to software tools such as Mathematica, however, the most commonly used Notebook format today is the Jupyter Notebook (formerly called the iPython notebook – hence the file extension ipynb). Jupyter is an amalgomation of Julia, Python, and R – the three programming languages it was originally designed for – however over the years a number of extensions have been developed to support many more languages, making it a defacto standard for computational notebooks. Notebooks essentially store their data in JSON format, and the Jupyter environment then renders this correctly for user interaction. More recently, Jupyter Lab has been proposed as a browser-based environment for working with notebooks that supports a number of modern features compared to the original Jupyter environment.\n",
    "\n",
    "![Alt text](./images/image7.png)\n",
    "\n",
    "Python has grown in popularity significantly over the last 10 years. There are a number of reasons, such as the ability to use the REPL command line (Read-Evaluate-Print-Loop), which links well with our view of iterative practice for data science. Furthermore, the wealth of libraries developed for Python mean that many tasks can be performed quickly using these libraries rather than coding from scratch. In particular, for Data Science, there are a number of libraries that are seen as the standard: NumPy (Numerical Python), SciPy (Scientific Python), Matplotlib (a Python graphing library originating from MATLAB conventions), Pandas (Powerful Data Analysis), as well as scikit-learn (used for machine learning), Tensorflow, Keras and PyTorch (used for deep learning), and NLTK (Natural Language Toolkit).\n",
    "\n",
    "In the hands-on video, and in the practical live session, we will focus on a “Hello Security Data Analytics” 101, to demonstrate how we can use a data science workflow with the Jupyter notebook environment to solve a cyber security challenge. The manager of the security operations centre (SOC) suggests that her analysts are becoming inundated with “trivial” alerts ever since a new data set of indicators was introduced into the Security Information and Event Management (SIEM) system. As a cyber security data scientist, they have asked for your help to reduce the number of “trivial” alerts without sacrificing visibility of security alerts. This is a good problem to tackle through data analysis, and we should be able to form a solid, practical question to ask after we perform some exploratory data analysis and hopefully arrive at an answer that can help the SOC. In the next session, we will look at how we can use Jupyter notebooks to help us solve this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reading\n",
    "\n",
    "- [JupyterLab Documentation](https://jupyterlab.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
